@MISC{Papamakarios14comparisonof,
    author = {George Papamakarios},
    title = {Comparison of Modern Stochastic Optimization Algorithms},
    year = {2014}
}

@inproceedings{Kotsiantis,
 author = {Kotsiantis, S. B.},
 title = {Supervised Machine Learning: A Review of Classification Techniques},
 booktitle = {Proceedings of the 2007 Conference on Emerging Artificial Intelligence Applications in Computer Engineering: Real Word AI Systems with Applications in eHealth, HCI, Information Retrieval and Pervasive Technologies},
 year = {2007},
 isbn = {978-1-58603-780-2},
 pages = {3--24},
 numpages = {22},
 url = {http://dl.acm.org/citation.cfm?id=1566770.1566773},
 acmid = {1566773},
 publisher = {IOS Press},
 address = {Amsterdam, The Netherlands, The Netherlands},
 keywords = {Classifiers, Data Mining, Intelligent Data Analysis, Learning Algorithms},
}

@Inbook{Bottou2010,
author="Bottou, L{\'e}on",
title="Large-Scale Machine Learning with Stochastic Gradient Descent",
bookTitle="Proceedings of COMPSTAT'2010: 19th International Conference on Computational StatisticsParis France, August 22-27, 2010 Keynote, Invited and Contributed Papers",
year="2010",
publisher="Physica-Verlag HD",
address="Heidelberg",
pages="177--186",
abstract="During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.",
isbn="978-3-7908-2604-3",
doi="10.1007/978-3-7908-2604-3_16",
url="https://doi.org/10.1007/978-3-7908-2604-3_16"
}

@article{Wright,
 author = {Wright, Stephen J.},
 title = {Coordinate Descent Algorithms},
 journal = {Math. Program.},
 issue_date = {June      2015},
 volume = {151},
 number = {1},
 month = jun,
 year = {2015},
 issn = {0025-5610},
 pages = {3--34},
 numpages = {32},
 url = {http://dx.doi.org/10.1007/s10107-015-0892-3},
 doi = {10.1007/s10107-015-0892-3},
 acmid = {2783189},
 publisher = {Springer-Verlag New York, Inc.},
 address = {Secaucus, NJ, USA},
 keywords = {49M20, 90C25, Coordinate descent, Parallel numerical computing, Randomized algorithms},
} 

@InProceedings{pmlrv37nutini15,
  title = 	 {Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than Random Selection},
  author = 	 {Julie Nutini and Mark Schmidt and Issam Laradji and Michael Friedlander and Hoyt Koepke},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1632--1641},
  year = 	 {2015},
  editor = 	 {Francis Bach and David Blei},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/nutini15.pdf},
  url = 	 {http://proceedings.mlr.press/v37/nutini15.html},
  abstract = 	 {There has been significant recent work on the theory and application of randomized coordinate descent algorithms, beginning with the work of  Nesterov [SIAM J. Optim., 22(2), 2012], who showed that a random-coordinate selection rule achieves the same convergence rate as the Gauss-Southwell selection rule. This result suggests that we should never use the Gauss-Southwell rule, as it is typically much more expensive than random selection. However, the empirical behaviours of these algorithms contradict this theoretical result: in applications where the computational costs of the selection rules are comparable, the Gauss-Southwell selection rule tends to perform substantially better than random coordinate selection. We give a simple analysis of the Gauss-Southwell rule showing that—except in extreme cases—it’s convergence rate is faster than choosing random coordinates. Further, in this work we (i) show that exact coordinate optimization improves the convergence rate for certain sparse problems, (ii) propose a Gauss-Southwell-Lipschitz rule that gives an even faster convergence rate given knowledge of the Lipschitz constants of the partial derivatives, (iii) analyze the effect of approximate Gauss-Southwell rules, and (iv) analyze proximal-gradient variants of the Gauss-Southwell rule.}
}

@techreport{Duchi,
    Author = {Duchi, John and Hazan, Elad and Singer, Yoram},
    Title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
    Institution = {EECS Department, University of California, Berkeley},
    Year = {2010},
    Month = {Mar},
    URL = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-24.html},
    Number = {UCB/EECS-2010-24},
    Abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods significantly outperform state-of-the-art, yet non-adaptive, subgradient algorithms.}
}

@article{DBLP,
  author    = {Matthew D. Zeiler},
  title     = {{ADADELTA:} An Adaptive Learning Rate Method},
  journal   = {CoRR},
  volume    = {abs/1212.5701},
  year      = {2012},
  url       = {http://arxiv.org/abs/1212.5701},
  archivePrefix = {arXiv},
  eprint    = {1212.5701},
  timestamp = {Wed, 07 Jun 2017 14:43:02 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/abs-1212-5701},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@incollection{NIPS20104006,
title = {Parallelized Stochastic Gradient Descent},
author = {Martin Zinkevich and Markus Weimer and Li, Lihong and Alex J. Smola},
booktitle = {Advances in Neural Information Processing Systems 23},
editor = {J. D. Lafferty and C. K. I. Williams and J. Shawe-Taylor and R. S. Zemel and A. Culotta},
pages = {2595--2603},
year = {2010},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4006-parallelized-stochastic-gradient-descent.pdf}
}

@inproceedings{Meng,
 author = {Meng, Qi and Chen, Wei and Yu, Jingcheng and Wang, Taifeng and Ma, Zhi-Ming and Liu, Tie-Yan},
 title = {Asynchronous Accelerated Stochastic Gradient Descent},
 booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
 series = {IJCAI'16},
 year = {2016},
 isbn = {978-1-57735-770-4},
 location = {New York, New York, USA},
 pages = {1853--1859},
 address = 	 {New York, USA},
 numpages = {7},
 url = {http://dl.acm.org/citation.cfm?id=3060832.3060880},
 acmid = {3060880},
 publisher = {AAAI Press},
} 

@ARTICLE{2012arXiv,
   author = {{Richt{\'a}rik}, P. and {Tak{\'a}{\v c}}, M.},
    title = "{Parallel Coordinate Descent Methods for Big Data Optimization}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1212.0873},
 primaryClass = "math.OC",
 keywords = {Mathematics - Optimization and Control, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
     year = 2012,
    month = dec,
   adsurl = {http://adsabs.harvard.edu/abs/2012arXiv1212.0873R},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
