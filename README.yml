---
owner:
    hid: 211
    name: Ajinkya Khamkar
    url: https://github.com/bigdata-i523/hid211
paper1:
    abstract: >
         Deep Neural networks are inherently parallel in nature. Research is
         generally focused on the going deeper paradigm. More recent research
         is concentrated on going wider instead of just going deep.  Wider paradigms
         have computational advantage since they, can be executed concurrently in
         a distributed environment. They do not possess the constraint of data sharing
         between concurrent blocks.
    author:
        - Khamkar, Ajinkya
    chapter: Machine Learning
    hid:
        - 211
    status: 10/27/2017 100%
    title: Distributed environment for neural network
    type: latex
    url: https://github.com/bigdata-i523/hid211/paper1/
paper2:
    review: Nov 6 2017
    abstract: Machine learning algorithms are computationally and time expensive. Optimizations are required to machine learning algorithms to improve their performance for big data.  
    author:
        - Khamkar, Ajinkya
    hid:
        - 211
    status: 11/06/2017 100%    
    title: Machine learning optimizations for big data
    type: report
    url: https://github.com/bigdata-i523/hid211/paper2/
